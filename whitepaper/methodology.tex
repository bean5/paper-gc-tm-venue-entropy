\section {Methodology}
First data is prepared by being canonicalized, stemmed, and stored. Then we use collapsed Gibbs sampling to infer parameters for our model, Latent Dirichlet Allocation. We divide our data into disjoint venues, then apply metrics on the estimated parameters of our model. We employ entropy to show how a venue narrows/broadens over time. This is graphed for easy interpretation. We normalize topic distributions for key time spans and graph the values of the resulting normalized topic probability mass over time. Lastly, we apply Jensen-Shannon divergence between pairs of venues to visualize how different/similar the venues are. This is also graphed over time for analysis.

\subsection{Data}
Discourses are taken from the LDS %\footnote{The Church of Jesus Christ of Latter-day Saints; members of said church are often referred to as ``Mormons'' and although they are okay with this nomenclature, it is a conflated term that is applied to offshoot religions that are very different from the original branch.}
General Conference (GC)\footnote{The LDS members convene during GC to hear from authorities and selected members of their church to be uplifted and edified together. Important announcements such as large-scale changes are diseminated in these conferences when appropriate. A recent change in minimum age for missionary service for the church is one example.} from 1942 to 2013. There are traditionally general conferences per year: one in April and one in October. Most sessions are considered general and are for meant for all members of the church, while some are gender-specific. Interestingly, sessions that are for the women in the church\footnote{``The Relief Society is the oldest and largest women's organization in the world. Relief Society was established in 1842 for women 18 years of age and older. Its purpose is to build faith and personal righteousness, strengthen families and homes, and help those in need.'' \cite{mormon.org-rs}} are generally held about 1 month prior to the general sessions. This means that the women's sessions are broadcasted in March and September. Women speakers are more prevalent, of course, at the women's sessions, so it is important that these discourses be in the dataset. For purposes of venue grouping, they are treated as having occurred 1 month later; this means that March women's sessions will be grouped with April general sessions, and likewise the September sessions will be grouped with the October general sessions. 

\subsection{Data Preparation}
Our dataset is by no means free of noise. Our texts are derived from over 70 years of broadcasted religious discourse. Over this time, language is sure to have changed (except for the majority of specialized religious words). The speakers of the church have varied in type of discourse as well (extemporaneous vs. prepared). Gender is sure to play a role in some of the `noisiness' of the data when viewed as a whole not only because of preferences in speech but because the probability that certain genders are given opportunity vary from year to year. To make matters worse, the data is in html format for the web and does have inconsistencies that have to be accounted for when being prepared for sampling. By preparing the data carefully for the algorithm, it is hoped that end result is a clearer picture of the noisy data. 

%By splitting the data into pre-defined venues, it is hoped that the information, particularly the topical distribution, can be more greatly appreciated. 

%A careful analysis of the trends and results will prove that although not free of noise, there is plenty of interesting information contained in religious documents, both old and new. Indeed, the fact that the probability mass of each topic will change over time is central.

%Although noisy at first, by separating which could otherwise be considered as noise that would confound models, can be separated and treated as a venue for further analysis hopefully allowing the data to speak for itself.

%Certainly there is room for topics to vary in popularity over the course of 73 years, but so will language, preference for discourse type (extemporaneous, prepared, long-quoted). 

\subsubsection{Format}
Discourses are stored as flat html files. HTML is removed by using custom scriptures written in NodeJS \cite{nodeJS}. Word order is not altered, although for LDA this is not important.%Although order doesn't matter for gibbs sampling over LDA, we maintain the original order 

\subsubsection{Canonicalization}
Since many of our documents are dated before the 19th century, punctuation is often inconsistent; we therefore opt to ignore it except to aid in parsing. We do this for all documents. Furthermore, we lower-case all text. Future work is viable here as numbers were eradicated from the corpus in spite of the possiblity that there might be interesting numbers in the dataset. 

\subsubsection{Stemming}
We do not use stemming as LDA can handle as it might cause conflation of lexemes. Although this means that more tokens exist, the topics will be more straight-forward to understand.
%We stem words using the Java 7 port of Java 6 Porter Stemmer which was available online \cite{github-bean5-porter7}; the Java 6 implementation was buggy in Java 7. In English, this stemmer allows words of different tenses to be simplified to a shorter, simpler form \cite{van1980new}. The porter stemmer algorithm is not perfect, but it is a straight-forward. A lemmatizer might perform better than a stemmer on this task, but we leave exploration in this space to future work. Beyond stemming and canonicalization, words are unchanged.

\subsubsection{Stopwording}
As is common in LDA models that whose parameters are estimated using gibbs sampling, we remove/ignore stopwords. We use the English stop-word list provided by Mallet \cite{McCallumMALLET}, but add the following words which appeared to counfound the model: ye, god, gods, jesus, christ. %Besides typical English stopwords such as `the' and `it', we ignore the following words while applying gibbs sampling: \textit{god, jesus, christ, father}. These words have term frequency--inverse document frequency (TF-IDF) \cite{wiki:tf-idf} scores that approach 1. These words were discovered by using Luke to perform queries over a Lucene index \cite{lucene:luke}.

%\begin{description}
%	\item [Level 0]: Word strings must match perfectly to be considered synonyms (i.e. synonymy is turned off)
%\end{description}

%\textit{WordNet} \cite{wordnet_1998} was used to query for obtaining synsets---including hypernyms and hyponyms. The WordNet morphology boolean was set to false for all queries. 

%Since part of speech tagging and word-sense disambiguation was not performed, the query for synsets always assumed words were nouns. Obviously, this is not the case, so there is definitely room for improvement in our method.
%Since this is not the case, we decrease \textit{recall} by blacklisting certain English words---particularly modal verbs. Our blacklist also included strings such as ``\&c.'' (etc.) which appeared to easily be counted as synonyms of words when in fact they were not. Our blacklist is as follows: \textit{has, can, had, might, would, should, will, shall, have....(xyz list all.)}


\subsection {Model}
We used a generative topic model known as Latent Dirichlet Allocation (LDA) \cite{Blei:2003:LDA:944919.944937}. It allows for multiple topics to be contained within a document by allowing each word instance (excluding stopwords) to be assigned 1 of K topics. This means that there is a distribution over topics for each document. LDA explicitly models a topical distribution over all words as well. %Although other models can be used to cluster documents, this work aims to cluster subsets of documents. such as those often used by EM, allowing for multiple topics per document seems more appropriate. Careful selection of algorithm will allow for the model to speak for the data.


\subsection {Algorithm}

\subsubsection{Collapsed Gibbs Sampling}
We use collapsed Gibbs sampling to infer the parameters of LDA \cite{Blei:2003:LDA:944919.944937}. Given hyperparameters and some parameters, the sampling algorithm outputs estimates for the unknown parameters (theta, phi). Collapsed Gibbs sampling provides only point-estimates of the parameters; this is sufficient for this work. 

\subsubsection{Hyperparameters}
For the number of topics, K, we start with 50, then try 100, then 200. We settle on 100 as it appeared to generate fairly coherent and intelligible topics. %Other parameters, such as length of document and tokens, are intrinsic to the dataset.

\subsubsection{Hyperparameters}
At first, hyperparameters allow for a uninformed distribution of topics. Using some options provided in the mallet toolkit, we allow hyperparameters to be re-estimated. This drives the likelihood of the data up, producing a better model.

\begin{comment}
\subsection{Data and Meta-data}
We view the data as being divisible into 2 types of meta-data before application of sampling, and as 1 type of meta-data after application. Although these are somewhat arbitrary/contrived categories, they are interesting ways to think of the data and are meant to help intuit future work.

\subsubsection{Intrinsic Meta-Data}
For each document, we have or can derive the following meta-data:
	\begin{itemize}
		\item \textit{date discourse was given};
		\item \textit{time of year: April/October};
		\item \textit{length of document}; and
		\item \textit{the text itself}.
	\end{itemize}

Although one can imagine building a model which conditions variables on such intrinsic data, similar to how one might condition a model on time or dialect---which is the case for \cite{Wang:2006:TOT:1150402.1150450} and \cite{crain2010dialect}, respectively---we do not condition variables in our model directly on any of this data. Instead, we use the same post-hoc method of Hall et al. in dividing the dataset after applying gibbs sampling. We will only divide data into venues based on gender and time of year. The possibility for future work which directly models more intrinsic data is viable.

\subsubsection{Extrinsic Meta-Data}
Although we do not directly leverage the use of any extrinsic \textbf{meta-data} in our research here, with the exception of speaker's gender, we believe it is important to note that it exists. Intuition hints that accounting for these variables in the model could be of benefit, just as accounting for dialect would be helpful. Extrinsic meta-data which we have available include:
	\begin{itemize}
		\item \textit{Name, age, gender of speaker}; and
		\item \textit{Speaker's Name}
		\item \textit{Speaker's Age}
		\item \textit{Speaker's Gender}
		\item \textit{Speaker's Church Position/Assignment}
		\item \textit{Date}
	\end{itemize}

\subsubsection{Postrinsic Data}
Upon applying gibbs sampling, we can obtain and/or estimate:
	\begin{itemize}
		\item \textit{topic assignments for each non-stopword of each document};
		\item \textit{topic distribution within each document};
		\item \textit{topic distribution within each venue};
		\item \textit{entropy of each venue for each session of GC \textbf{overall and for each year}}; and
		\item \textit{Jensen Shannon divergence between each pair of venues, described in section \ref{venues} \textbf{for each year}}.
	\end{itemize}
\end{comment}

\subsection{Metrics: Post-hoc Analysis}
Similar to \cite{hall-jurafsky-manning:2008:EMNLP}, we use the metrics described in Table \ref{tab:metric_purposes}. These metrics allow us to show how a topic moves over time and how a venue is different/similar to another.

\begin{table}[center]
	\centering
	\begin{tabular} {| l | l | l |}
		\hline	\textbf{Metric} 		& \textbf{Purpose} & \textbf{Notation} \\ \hline
		\hline	Probability of occurence	& show how topics trend from year to year;	& $p(topic|venue,year)$	\\
		\hline	Entropy				& show how some venues broaden/shrink over time	& $H(C)$ \\
		\hline	Jensen-Shannon Divergence	& compare pair-wise venues	& $JS(C, C')$\\
		\hline
	\end{tabular}
	\caption{Metrics and Purposes}
	\label{tab:metric_purposes}
\end{table}

\subsubsection{Venue Grouping}
\label{venues}
Following the methodology of Hall et al. we group our data according to venue. Following are the disjoint venues:

	\begin{description}
		\item [gender venues] female/male
		\item [semi-annual venues] April/October
%		\item [time-gender venues] female April, female October, male April, male October
	\end{description}

\subsubsection{Entropy Over Time}
For sake of simplicity, the venues are all graphed according to the the largest time span in common: year.

By graphing entropy over time, we get a sense of the general trend for each venue with respect to topic diversity. Lower entropy indicates a venue is less diverse than another. Following is the formula for entropy:

\begin{center}
$H(T) = - \sum_{t \in T} \;  p(t) \log p(t)$, where $T$ is a distribution in the probability simplex and $t$ is a topic.
\end{center}

\subsubsection{Topic Probability Mass Over Time}
In each venue there is a distribution over topics. Like entropy, this can be graphed. This allows us to visualize how each topic varies in terms of popularity over time.

This is obtained by averaging distributions for each document within the venue. 

\subsubsection{Jensen-Shannon Divergence Over Time}
Since we have the probability distribution for topics in each venue, we can also compute a divergence between pairs of venues. This can also be graphed over time, allowing us to visualize the divergence. We use Jensen Shannon divergence (JS)\footnote{JS divergence is the symmetrical corrollary of Kullback-Leibler divergence}. Divergence values are proportional to the difference of the probability distributions. Thus a high divergence would indicate that two venues are different. Below is the formula for JS divergence, given two probability distributions, $P, Q$ where $M$ is the average of the two. This value is computed for each \textit{unique} pair-wise comparison between documents of each venue, then is averaged together. Although \cite{hall-jurafsky-manning:2008:EMNLP} did not mention doing this, it seems appropriate to do so.

\begin{center}
$JS(P||Q) = 0.5 * KL(P||M) + KL(Q||M)$

where $KL(P||Q) = \sum\limits_{i=1}^n \ln(\frac{P(i)}{Q(i)})$
\end{center}
We define $KL(P||Q)$ to be zero when $Q(i)=0$.


%Although we do not aim to prove the merits of Gibbs sampling on LDA, we do want to impress upon the reader that goodness of the resulting estimations. For this task, we leverage existing classifiations as described in \ref{gold_standard} to compute an accuracy based hard assignments of topics. Although this is not optimal, it is the only clear way to leverage current gold-standard with the estimations that Gibbs outputs.

%\subsection{Gold Standard}
%\label{gold_standard}
%Online, the LDS church has published a concordance of discourses and topics for the latest 5 conferences (ending at the October 2013 session). Although these topics tend to be at most 1 per document, we can force hard assignments by choosing the most common gibbs-estimated topic within each document. These are then used to compute an accuracy. Since this is novel work and we are not attempting to prove the goodness of gibbs itself, but rather the goodness of our estimated model, there is little recourse but to compute an accuracy based on the sparse, but approved and accepted classifications available online \cite{lds.org}.
